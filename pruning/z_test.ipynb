{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deit_token_drop_variant_i import DeiTForImageClassificationDropTokens\n",
    "from transformers import DeiTForImageClassification\n",
    "from transformers import DeiTImageProcessor\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "import math\n",
    "from inference_patchers_blockprune import optimize_model_deit\n",
    "\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "drop_token_sparse_args = yaml.safe_load(Path(\"trained-models/imagenet-1k_deit_drop_tokens_variant_i_small/epochs_11_blockPruningInfo_finalThreshold_0.5_blockSize_32_method_topK_tokenDropInfo_layerCount_3_layerType_default_keepRate_0.5_fused_True_20231227-041334/sparse_training_args.yaml\").read_text())\n",
    "\n",
    "drop_token_model = DeiTForImageClassificationDropTokens.from_pretrained(pretrained_model_name_or_path=\"trained-models/imagenet-1k_deit_drop_tokens_variant_i_small/epochs_11_blockPruningInfo_finalThreshold_0.5_blockSize_32_method_topK_tokenDropInfo_layerCount_3_layerType_default_keepRate_0.5_fused_True_20231227-041334/fine-pruned-MASKED\")\n",
    "baseline_model = DeiTForImageClassification.from_pretrained(pretrained_model_name_or_path=\"trained-models/deit-small-baseline-imagenet-1k-20240110-014420/best-baseline-loaded-at-end\")\n",
    "\n",
    "drop_token_config = drop_token_model.config\n",
    "baseline_config = baseline_model.config\n",
    "\n",
    "drop_token_image_processor = DeiTImageProcessor.from_pretrained(pretrained_model_name_or_path=\"trained-models/imagenet-1k_deit_drop_tokens_variant_i_small/epochs_11_blockPruningInfo_finalThreshold_0.5_blockSize_32_method_topK_tokenDropInfo_layerCount_3_layerType_default_keepRate_0.5_fused_True_20231227-041334/fine-pruned-MASKED\")\n",
    "baseline_image_processor = DeiTImageProcessor.from_pretrained(pretrained_model_name_or_path=\"trained-models/deit-small-baseline-imagenet-1k-20240110-014420/best-baseline-loaded-at-end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "mpca = MPCA(t=12,\n",
    "            c=2,\n",
    "            h=4,\n",
    "            p_pe=8,\n",
    "            frequency=300)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_drop_token_model = optimize_model_deit(model=drop_token_model,\n",
    "                                                 mode=\"dense\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds = load_dataset(\"imagenet-1k\", split=['train', 'validation'])   # Validation is TEST\n",
    "\n",
    "splitter_to_train_val = train_ds.train_test_split(test_size=25000)    # 25k images from train as part of val            \n",
    "\n",
    "train_ds = splitter_to_train_val['train']       # 1256167    instances\n",
    "val_ds = splitter_to_train_val['test']          # 25000      instances\n",
    "\n",
    "key_to_get_image = 'image'\n",
    "\n",
    "\n",
    "\n",
    "image_mean, image_std = drop_token_image_processor.image_mean, drop_token_image_processor.image_std                 \n",
    "size = drop_token_image_processor.crop_size[\"height\"]                                             \n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(size),\n",
    "            CenterCrop(size),                                             \n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples[key_to_get_image]]\n",
    "    return examples\n",
    "\n",
    "test_ds.set_transform(val_transforms) \n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "inputs = collate_fn([test_ds[i] for i in range(1)])\n",
    "\n",
    "outputs = baseline_model(**inputs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pruning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
