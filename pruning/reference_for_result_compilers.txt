################################################################################################################################
FOR DeiT Drop Tokens
################################################################################################################################


################
sparse_args
################


{'Token_Drop_Layer_Wise_Info': {2: {'fuse_inattentive': True, 'keep_rate': 0.5}, 6: {'fuse_inattentive': True, 'keep_rate': 0.5}, 9: {'fuse_inattentive': True, 'keep_rate': 0.5}}, 'ampere_pruning_method': 'disabled', 'attention_block_cols': 32, 'attention_block_rows': 32, 'attention_lambda': 1.0, 'attention_output_with_dense': False, 'attention_pruning_method': 'topK', 'bias_mask': True, 'decoder_attention_lambda': None, 'decoder_dense_lambda': None, 'dense_block_cols': 1, 'dense_block_rows': 1, 'dense_lambda': 1.0, 'dense_pruning_method': 'topK:1d_alt', 'distil_alpha_ce': 0.1, 'distil_alpha_teacher': 0.9, 'distil_teacher_name_or_path': 'google/vit-base-patch16-224', 'distil_temperature': 2.0, 'eval_with_current_patch_params': False, 'final_ampere_temperature': 20.0, 'final_finetune': False, 'final_threshold': 0.5, 'final_warmup': 10, 'gelu_patch': False, 'gelu_patch_steps': 50000, 'initial_ampere_temperature': 0.0, 'initial_threshold': 1.0, 'initial_warmup': 1, 'layer_norm_patch': False, 'layer_norm_patch_start_delta': 0.99, 'layer_norm_patch_steps': 50000, 'linear_min_parameters': 0.005, 'mask_init': 'constant', 'mask_scale': 0.0, 'mask_scores_learning_rate': 0.01, 'qat': False, 'qconfig': 'default', 'regularization': 'disabled', 'regularization_final_lambda': 0.0, 'rewind_model_name_or_path': None}



################
config
################


DeiTConfigDropTokens {
  "_name_or_path": "trained-models/imagenet-1k_deit_drop_tokens_variant_i_small/epochs_11_blockPruningInfo_finalThreshold_0.5_blockSize_32_method_topK_tokenDropInfo_layerCount_3_layerType_default_keepRate_0.5_fused_True_20231227-041334/fine-pruned-MASKED",
  "architectures": [
    "DeiTForImageClassificationDropTokens"
  ],
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "fuse_inattentive_TD": [
    true,
    true,
    true
  ],
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 384,
  "id2label": <redacted_for_readability>,
  "image_size": 224,
  "initializer_range": 0.02,
  "intermediate_size": 1536,
  "keep_rate_TD": [
    0.5,
    0.5,
    0.5
  ],
  "label2id": <redacted_for_readability>,
  "layer_indices_TD": [
    2,
    6,
    9
  ],
  "layer_norm_eps": 1e-12,
  "model_type": "deit",
  "num_attention_heads": 6,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "problem_type": "single_label_classification",
  "pruned_heads": {
    "0": [
      0
    ],
    "1": [
      3
    ],
    "2": [
      1,
      2
    ],
    "3": [
      4
    ],
    "4": [
      3
    ],
    "5": [
      4
    ],
    "6": [
      0,
      4
    ],
    "7": [
      5
    ],
    "8": [],
    "9": [
      4
    ],
    "10": [],
    "11": []
  },
  "qkv_bias": true,
  "torch_dtype": "float32",
  "transformers_version": "4.32.1"
}

